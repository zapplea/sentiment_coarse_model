import tensorflow as tf
import numpy as np
from data_generator import DataGenerator


class AttributeFunction:
    def __init__(self, nn_config):
        self.nn_config = nn_config

    def attribute_vec(self, graph):
        # A is matrix of attribute vector
        A = []
        for i in range(self.nn_config['attributes_num']):
            att_vec = tf.get_variable(name='att_vec' + str(i),
                                      initializer=tf.random_uniform(shape=(self.nn_config['attribute_dim'],),
                                                                    dtype='float32'))
            A.append(att_vec)
        graph.add_to_collection('A', A)
        o = tf.get_variable(name='other_vec', initializer=tf.random_uniform(shape=(self.nn_config['attribute_dim'],),
                                                                            dtype='float32'))
        graph.add_to_collection('o', o)
        return A, o

    def attribute_mat(self, graph):
        # all attribute mention matrix are the same, but some a padded with 0 vector. We use mask to avoid the training process
        A_mat = []
        for i in range(self.nn_config['attributes_num']):
            att_mat = tf.get_variable(name='att_mat' + str(i),
                                      initializer=tf.random_uniform(shape=(self.nn_config['attribute_mat_size'],
                                                                           self.nn_config['attribute_dim']),
                                                                    dtype='float32'))
            A_mat.append(att_mat)
        graph.add_to_collection('A_mat', A_mat)
        o_mat = tf.get_variable(name='other_vec',
                                initializer=tf.random_uniform(shape=(self.nn_config['attribute_mat_size'],
                                                                     self.nn_config['attribute_dim']),
                                                              dtype='float32'))
        graph.add_to_collection('o_mat', o_mat)

        return A_mat, o_mat

    def attribute_mat_attention(self, att_mat, word_embed, graph):
        """

        :param att_mat: 
        :param word_embed: 
        :param graph: 
        :return: shape=(attribute matrix size, word dim),  attribute dim is the same to word dim
        """
        attention = tf.nn.softmax(tf.reduce_sum(tf.multiply(att_mat, word_embed), axis=1))
        attention = tf.expand_dims(attention, axis=1)
        attention = tf.tile(attention, multiples=[1, self.nn_config['word_dim']])
        graph.add_to_collection('att_mat_attention', attention)
        return attention

    def attribute_mat2vec(self, word_embed, A_mat, o_mat, graph):
        # A is matrix attribute matrix
        A = []
        for att_mat in A_mat:
            attention = self.attribute_mat_attention(att_mat, word_embed, graph)
            att_vec = tf.reduce_sum(tf.multiply(attention, att_mat), axis=0)
            A.append(att_vec)
        graph.add_to_collection('A', A)
        o_attention = self.attribute_mat_attention(o_mat, word_embed, graph)
        o = tf.reduce_sum(tf.multiply(o_attention, o_mat), axis=0)
        graph.add_to_collection('o', o)
        return A, o

    def words_attribute_mat2vec(self, x, A_mat, o_mat, graph):
        """
        convert attribtes matrix to attributes vector for each words in a sentence
        :param x: 
        :param A_mat: 
        :param o_mat: 
        :param graph: 
        :return: shape = (number of words, number of attributes, attribute dim(=word dim))
        """
        words_A = []
        for i in range(self.nn_config['words_num']):
            word_embed = x[i]
            A, o = self.attribute_mat2vec(word_embed, A_mat, o_mat, graph)
            words_A.append((A, o))
        graph.add_to_collection('words_attributes', words_A)
        return words_A

    def score(self, A_mat, o_mat, x, graph):
        """
        since x is just a sentence ,not a batch, so the A and o should not be generated by this function. otherwise, each time will generate a new A 
        :param A_mat: 
        :param o_mat: 
        :param x: a sentence 
        :param graph: 
        :return: [att1_score,att2_score,...,attk_score]; attj_score is a scalar.
        """
        if not self.nn_config['is_mat']:
            A = tf.subtract(A_mat, o_mat)
            result = tf.reduce_max(tf.matmul(A, x, transpose_b=True), axis=1)
        else:
            temp = []
            for i in range(self.nn_config['words_num']):
                word_embed = x[i]
                A = A_mat[i]
                o = o_mat[i]
                A = tf.subtract(A, o)
                temp.append(tf.reshape(tf.matmul(tf.expand_dims(word_embed, axis=0), A, transpose_b=True),
                                       shape=(self.nn_config['attributes_num'],)))
            result = tf.reduce_max(tf.transpose(temp), axis=1)
        graph.add_to_collection('att_score', result)
        return result

    def prediction(self, score, graph):
        condition = tf.greater(score, tf.ones_like(score, dtype='float32') * self.nn_config['atr_threshold'])
        pred = tf.where(condition, tf.ones_like(score, dtype='float32'), tf.zeros_like(score, dtype='float32'))
        graph.add_to_collection('atr_pred', pred)
        return pred

    def max_f_score(self,score,atr_label):
        condition = tf.equal(tf.ones_like(atr_label, dtype='float32'), atr_label)
        max_fscore = tf.reduce_max(tf.where(condition,
                                            tf.ones_like(atr_label, dtype='float32') * tf.constant(-np.inf, dtype='float32'),
                                            score))
        # consider when a sentence contains all attributes
        max_fscore = tf.where(tf.is_inf(max_fscore),tf.zeros_like(max_fscore,dtype='float32'),max_fscore)
        return max_fscore


    def loss(self, score, pred, atr_label, graph):
        """
        
        :param score: 
        :param pred: 
        :param atr_label: shape = (number of attributes,) 
        :param graph: 
        :return: 
        """
        # max wrong score
        # extract the max false attributes score
        max_fscore = self.max_f_score(score,atr_label)

        theta = tf.constant(self.nn_config['attribute_loss_theta'], dtype='float32')
        loss = tf.add(tf.subtract(theta, tf.multiply(atr_label,score)), max_fscore)

        # non-attribute and attribute use different mask
        condition = tf.equal(tf.reduce_sum(atr_label),0)
        nonatr_label = tf.zeros_like(atr_label, dtype='float32')
        nonatr_label[0] = 1
        mask = tf.cond(condition,lambda : nonatr_label,lambda : atr_label)
        # mask loss so that the false label will not be updated.
        masked_loss = tf.multiply(mask, loss)

        # max{0,loss}
        zero_item = tf.zeros_like(atr_label,dtype='float32')
        zero_item = tf.expand_dims(zero_item,axis=1)
        loss = tf.expand_dims(masked_loss,axis=1)
        loss = tf.reduce_max(tf.concat([zero_item,loss],axis=1),axis=1)
        loss = tf.reduce_sum(loss)
        graph.add_to_collection('atr_loss', loss)
        return loss

class Classifier:
    def __init__(self, nn_config):
        self.nn_config = nn_config
        self.dg = DataGenerator(nn_config)
        self.af = AttributeFunction(nn_config)
        self.dg = DataGenerator()  # should give argument to DataGenerator

    def sentences_input(self, graph):
        X = tf.placeholder(
            shape=(self.nn_config['batch_size'], self.nn_config['sentence_words_num'], self.nn_config['word_dim']),
            dtype='float32')
        graph.add_to_collection('X', X)
        return X

    def attribute_labels_input(self, graph):
        y_att = tf.placeholder(shape=(self.nn_config['batch_size'], self.nn_config['attributes_num']), dtype='float32')
        graph.add_to_collection('y_att', y_att)
        return y_att


    # should use variable share
    def sentence_lstm(self, X, graph):
        """
        return a lstm of a sentence
        :param X: sentences in a review
        :param graph: 
        :return: 
        """
        weight = tf.get_variable(name='sentence_lstm_w',
                                 initializer=tf.random_uniform(shape=(self.nn_config['word_dim'],
                                                                      self.nn_config['lstm_cell_size']),
                                                               dtype='float32'))
        bias = tf.get_variable(name='sentence_lstm_b',
                               initializer=tf.zeros(shape=(self.nn_config['lstm_cell_size']), dtype='float32'))

        X = tf.reshape(X, shape=(-1, self.nn_config['word_dim']))
        Xt = tf.add(tf.matmul(X, weight), bias)
        Xt = tf.reshape(Xt, shape=(-1, self.nn_config['words_num'], self.nn_config['lstm_cell_size']))
        # xt = tf.add(tf.expand_dims(tf.matmul(x, weight), axis=0), bias)
        cell = tf.nn.rnn_cell.BasicLSTMCell(self.nn_config['lstm_cell_size'])
        init_state = cell.zero_state(batch_size=self.nn_config['batch_size'], dtype='float32')
        # outputs.shape = (batch size, max_time, cell size)
        outputs, _ = tf.nn.dynamic_rnn(cell, inputs=Xt, initial_state=init_state, time_major=False)
        graph.add_to_collection('sentence_lstm_outputs', outputs)
        return outputs

    def optimizer(self, joint_losses, graph):
        opt = tf.train.AdamOptimizer(self.nn_config['lr']).minimize(tf.reduce_mean(joint_losses))
        graph.add_to_collection('opt', opt)
        return opt

    def lookup_table(self, X, graph):
        """
        :param X: shape = (batch_size, words numbers)
        :return: shape = (batch_size, words numbers, word dim)
        """
        table = self.dg.table_generator()
        table = tf.Variable(np.array(table), name='table')
        embeddings = tf.nn.embedding_lookup(table, X, partition_strategy='mod', name='lookup_table')
        graph.add_to_collection('lookup_table', embeddings)
        return embeddings

    def classifier(self):
        graph = tf.Graph()
        with graph.as_default():
            X = self.sentences_input(graph=graph)
            X = self.lookup_table(X, graph)
            # lstm
            with tf.variable_scope('sentence_lstm'):
                # H.shape = (batch size, max_time, cell size)
                H = self.sentence_lstm(X, graph=graph)

            y_att = self.attribute_labels_input(graph=graph)
            if not self.nn_config['is_mat']:
                A, o = self.af.attribute_vec(graph)
            else:
                A, o = self.af.attribute_mat(graph)
        joint_losses=[]
        for i in range(self.nn_config['batch_size']):
            with graph.as_default():
                # x.shape=(words number, word dim)
                x = X[i]
                h = H[i]
                atr_label = y_att[i]
                # if the attribute is represented by a mat, we need to convert it to a vector based on a word
                if self.nn_config['is_mat']:
                    words_A_o = self.af.words_attribute_mat2vec(x=x, A_mat=A, o_mat=o, graph=graph)
                    # shape = (number of words, number of attributes, attributes dim = words dim)
                    A = []
                    o = []
                    for l in range(len(words_A_o)):
                        A.append(words_A_o[l][0])
                        o.append(words_A_o[l][1])

            # sentiment extractor for attributes at a sentence.
            # attribute function
            with graph.as_default():
                atr_score = self.af.score(A_mat=A, o_mat=o, x=x, graph=graph)
                pred = self.af.prediction(score=atr_score, graph=graph)
                atr_loss = self.af.loss(atr_score, pred, atr_label, graph)
                joint_losses.append(atr_loss)

        with graph.as_default():
            opt = self.optimizer(joint_losses=joint_losses, graph=graph)
            saver = tf.train.Saver()
        return graph, saver

    def train(self):
        graph, saver = self.classifier()
        with graph.as_default():
            # input
            X = graph.get_collection('X')
            # labels
            y_att = graph.get_collection('y_att')
            y_senti = graph.get_collection('y_senti')
            # train_step
            train_step = graph.get_collection('train_step')
            # attribute function
            init = tf.global_variables_initializer()
        with graph.device('/gpu:1'):
            with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
                sess.run(init)
                for i in range(self.nn_config['epoch']):
                    sentences, att_labels, senti_labels = self.dg.gen(i) + 3
                    senti_extors = self.sentiment_extract_mat()
                    sess.run(init)
                    sess.run(train_step, feed_dict={X: sentences, y_att: att_labels, y_senti: senti_labels})