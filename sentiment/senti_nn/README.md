#1. instruction of program
###1.1 input batch size:
there is no limitation to the size of batch. You can change code in sentinn_train.
###1.2 prediction and loss
we use softmax cross entropy to construct the loss and use result of softmax to predict the labels.
###1.3 relative distance and dependency path
When we recognize a sentiment word in sentence, we don't know to which attribute it belongs, so, by calculating the distance between sentiment word and 
attribute word, we want to associate the sentiment word to attribute words.
###1.4 how to start
1. use util/dependency_parser/dependency_generator.py to generate a "dependency path table" and "sentences" (look at section 5)
all three versions of classifiers use the sentences generated by this program as input. In these sentences, "punctuation" is deleted by the 
Stanford Dependency Parser automatically, so to be convenient, we need to use the sentences generated by it.

2. run classifier.py in each folder. You can see the necessary input at section 3.

#2. Metrics
You can write a new metrics program. 
In the classifier, the final output is prediction result with shape (batch size, attributes number+1, 3).

#3. Data:
###3.1 sentiment labels:
shape = (batch size, attributes number+1, 3). The non-attribute should be considered and its the last attribute. 
batch= [sentiment for sentence1, ...]
sentiment for sentence1 = [sentiment for attribute1, ..., sentiment for non-attribute]
sentiment for attribute1 = [0,0,1]
The first represent neutral, the second represents negative, the third is positive. When a sentiment is true, then the corresponding position will be 1.
If an attribute(including non-attribute) doesn't appear in a sentence, then its sentiment is neutral. 

###3.2 attributes labels:
The same to the attribute function, and the shape is (batch size, attributes number). 
I will add one position in the end of each vector to represent non-attribute by the program, and the shape will be (batch size, attributes number +1). 
This prograss is finished by the program so don't worry.

###3.3 sentences:
The same to the attribute function in current state, but in the future, we would use the version without punctuation because of dependency parsing.

###3.4 wordembedding table
The same to attribute function

###3.5 dependency path
Need to input dependency path of each sentence. These two things will be generated by util/dependency_parser/dependency_generator.py

#4. Wordembedding:
In the dependency parsing version, the relationship words in sentiment path should be involved. Need to check how many words is unk in the generated sentence.

#5. path dependency version
### relation words
dependency relathion words: since these words are special and cannot be find in the original word embedding, we extract all of them and form a vocabulary of 
relation words. The relation words are something like "#case#", or "#amod#". They will be randomly initialized, which means we use np.random.normal to give each 
word a embedding. Then this vocabulary will be attached at the end of the original word embeddings, so we can use one lookup table to process sentences and 
dependency path.
In the program, just need to know the number of relation words.

#6. paramters
###6.1 new paramters in dependency path
These parameter should also be included in relative distance version

'rel_words_num': the number of relation words.
'rel_word_dim': dimension of relation words' embedding. should be the same to original word embeddings' dimension.
'max_path_length': the length of path dependency. The generator will produce it.

###6.2 constraints
lstm cell size = word dim
rel_words_dim = lstm cell size
rp_dim = lstm cell size

###6.3 explanation of each paramter
'attributes_num': number of attributes

'attribute_senti_prototype_num': 10, number of sentiment prototypes for a specific attribute

'normal_senti_prototype_num': 10,  # number of sentiment prototypes for each normal sentiment polarity like "Negative"

'sentiment_dim': seed['lstm_cell_size'],  # dim of a sentiment expression prototype.

'attribute_dim': seed['lstm_cell_size'], # dim of an attribute

'attribute_mat_size': 3,  # number of attribute mention prototypes in a attribute matrix

'words_num': 10, # number of words

'word_dim': seed['word_dim'], # word dimension

'rel_words_num':20, # number of dependency-parser relation words

'rel_word_dim':seed['word_dim'], # a relation word's dimension

'attribute_loss_theta': 1.0, # deprecated

'sentiment_loss_theta': 1.0, # deprecated 

'is_mat': False, # if the attribute mention is a matrix

'epoch': None,

'rps_num': 5,  # number of relative distance. if it is 5, then it means , for word_i, the distance between the other words and word_i is at most 5.
if the distance is greater than 5, then we still consider the distance as 5.

'rp_dim': seed['lstm_cell_size'],  # dimension of relative distance embedding

'lr': 0.003,  # learing rate 

'batch_size': 30,

'lstm_cell_size': seed['lstm_cell_size'],

'atr_threshold': 0,  # attribute score threshold, not used in here

'reg_rate': 0.03, # regularization rate

'senti_pred_threshold':0, # deprecated, used to predict sentiment

'lookup_table_words_num':2981402, # number of words in the original word embeddings. Don't need to include relation word

'padding_word_index':1, # index of padding word in the vocabulary

'max_path_length':20 # the maximal length of dependency path, the util/dependency_parser/dependency_generator.py will print it out.

#7. generate dependency path
Use util/dependency_parser/dependency_generator.py to generate sentences (without punctuation) and dependency path.
It will generate two json file: train.json and test.json. 
In each file, it contains two elements: 'encoded_tables' and 'encoded_sentences'. 
The e'ncoded_sentences' contain all sentences in test/train dataset. Words are converted to the number in the  wordembedding.
In the 'encoded_tables', it contains each sentence's dependency path.

###7.1 padding
There are two place need to be padded:

1. The length of dependency path are different, so we need to pad them to the same length.

2. The length of sentence are different, so the numbers of path generated by each sentence are different, so we need to pad it.

###7.2 strucutre of dependency path
shape = (batch size, words number, words number, dependency path length)
batch size is the number of sentences; 
In a sentence, each word will generate k dependency path and k words will generate k*k dependency path;
dependency path length is the length of dependency path.

dependency path for a batch of sentences: [sentence_j dependency path, ...]
sentence_j dependency path = [..., word_i's dependency path table, ...]
word_i's dependency path table = [..., word_i to word_k's dependency path, ...]
word_i to word_k's dependency path = last hidden state of bilstm

#Note:
The attribute is given even in the test. So, if the attributes cannot be recognized correctly, the performance of the sentiment will be influenced.
TODO:for coarse model, there should be something to eliminate the influence of padded sentences' labels.












